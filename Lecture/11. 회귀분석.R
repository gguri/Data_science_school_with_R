##############################################################
### 수업내용 : 회귀 분석                                   ###
##############################################################

## 언제 : 인과관계(원인과 결과)
##      : 예측(Prediction), 분류(Classification)

## 종속변수 = 반응변수 ****
## 독립변수 = 설명변수

## 단순선형회귀분석 : 종속변수 1개(양적), 독립변수 1개
## 다중선형회귀분석 : 종속변수 1개(양적), 독립변수 2개 이상

##########################
## 1. 단순선형회귀분석  ##
##########################

## 예제 데이터 : cars
## 변수명      : speed, dist

## 종속변수 : dist
## 독립변수 : speed

## 회귀모형 : dist = beta0 + beta1*speed + error
## 제동거리는 차마다 전부다 다른데 이 '다름'은 왜 발생했을까?

## 회귀분석 결과물 = lm(종속변수 ~ 독립변수, data=)
## summary(회귀분석 결과물)
cars.lm = lm(dist ~ speed, data = cars)
summary(cars.lm)
## Residuals(잔차) : 실제값 - 추정식 => 0에 가까울수록 좋은 것

#####################################
## 회귀분석의 결과를 해석하는 방법 ##
#####################################

## 1단계 : 회귀모형은 통계적으로 타당한가?
## 귀무가설 : 회귀모형은 타당하지 않다.
## 대립가설 : 회귀모형은 타당하다.
## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
## 분산분석은 SST = SSE(집단내다름)+SSB(집단간다름)
## 회귀분석은 SST = SSE+SSR
## SSR/자유도 / SSE/자유도 = MSR / MSE = F

## 1단계 결론 : 대립가설(회귀모형은 통계적으로 타당하다.)


## 2단계 : (1단계의 결론이 대립가설일 때)
## 독립변수는 종속변수에 영향을 주는가?
## 귀무가설 : 독립변수는 종속변수에게 영향을 주지 않는다.
## 대립가설 : 독립변수는 종속변수에게 영향을 준다.

##            Estimate Std.  Error    t value   Pr(>|t|)    
##  speed         3.9324     0.4155    9.464    1.49e-12 ***

## t value
## Ho : beta1 = 0 (기울기가 0)
## H1 : beta1 != 0
## beta1_hat = b1 = 0.002 ~ t분포(****)  나온 값의 분포를 확인해라

## 2단계 결론 : 유의확률이 0.000이므로 유의수준 0.05에서 
##              독립변수는 종속변수에게 (통계적으로 유의한) 영향을 준다.


## 3단계 : 독립변수는 종속변수에게 어떠한 영향을 주는가?
##         Estimate Std.
##  speed    3.9324  

## 독립변수의 회귀계수(Coefficient of Regression) : 3.932
## 독립변수의 기본단위가 1 증가하면
## 종속변수는 약 3.932 정도 증가한다.
## '단위'를 써줘야 한다. ****
## speed가 1mph 증가하면 
## distance가 약 3.932 feet 증가한다.


## 4단계 : 예측(prediction)
## dist = -17.5791 + 3.9324*speed
## predict(회귀분석결과, 
##         newdata=data.frame(speed=),
##         interval=)
predict(cars.lm, 
        newdata = data.frame(speed=200),
        interval = "predict")
## interval => 구간추정 / 없으면 점추정


## 5단계 : 회귀모형의 설명력 = 독립변수의 설명력
## dist가 다른데 speed가 과연 몇%만큼 설명하나? => R-squared(결정계수)
## R-squared:  0.651 => 0.651*100(%) => 65.1%
## speed가 dist의 총제곱합(다름)을 약 65.1% 설명한다.
## Adjusted R-squared:  0.643


##########################
## 2. 다중선형회귀분석  ##
##########################

## 종속변수 1개      : 양적 자료
## 독립변수 2개 이상 : 양적 자료, 질적 자료

## 예제 데이터 : attitude
## 종속변수    : rating
## 독립변수    : 6개

## 회귀모형 : rating = beta0 + 
##                     beta1*complaints + ... + beta6*advance +
##                     error

## 회귀분석결과 = lm(rating ~ complaints + ... + advance, data=데이터명)
## 회귀분석결과 = lm(rating ~ ., data=데이터명)
## summary(회귀분석결과)

attitude.lm = lm(rating ~ ., data = attitude)
summary(attitude.lm)

#####################################
## 회귀분석의 결과를 해석하는 방법 ##
#####################################

## 1단계 : 회귀모형은 통계적으로 타당한가?
## 귀무가설 : 회귀모형은 타당하지 않다.
## 대립가설 : 회귀모형은 타당하다.

## F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05
## 분산분석은 SST = SSE(집단내다름)+SSB(집단간다름)
## 회귀분석은 SST = SSE+SSR
## SSR/자유도 / SSE/자유도 = MSR / MSE = F

## 1단계 결론 : 대립가설(회귀모형은 통계적으로 타당하다.)


## 2단계 : (1단계의 결론이 대립가설일 때)
## 독립변수 각각은 종속변수에 영향을 주는가?
## 귀무가설 : 독립변수 각각은 종속변수에게 영향을 주지 않는다.
## 대립가설 : 독립변수 각각은 종속변수에게 영향을 준다.

##            Estimate Std.  Error     t value    Pr(>|t|)    
## complaints    0.61319     0.16098    3.809    0.000903 ***
## privileges   -0.07305     0.13572   -0.538    0.595594    
## learning      0.32033     0.16852    1.901    0.069925 .  
## raises        0.08173     0.22148    0.369    0.715480    
## critical      0.03838     0.14700    0.261    0.796334    
## advance      -0.21706     0.17821   -1.218    0.235577 

## 2단계 결론 : 유의확률이 0.000이므로 유의수준 0.05에서 
##              독립변수는 종속변수에게 (통계적으로 유의한) 영향을 준다.

## 변수선택 방법
## 1. 전진 선택법(Forward Selection)
##     => 종속변수와 가장 영향도가 높은 것이 모형에 들어감
##     => 추가적으로 기준에 만족하는 변수들을 계속 넣어줌
##     => 모형에 들어간 변수는 제거되지 않는다.
## 2. 후진 제거법(Backward Elimination)
##     => 종속변수와 가장 영향도가 없는 것이 모형에서 빠짐
##     => 추가적으로 기준에 만족하는 않는 변수들을 계속 제거
##     => 모형에 들어간 변수는 포함되지 않는다.
## 3. 단계 선택법(Stepwise Selection)
## 4. 독립변수 다 들어가는 것

## step(회귀분석결과, direction = c("forward","backward","both"))
model.forward  = step(attitude.lm,
                      direction = c("forward"))
model.backward = step(attitude.lm,
                      direction = c("backward"))
model.stepwise = step(attitude.lm,
                      direction = c("both"))
summary(model.stepwise)

model.final = lm(rating ~ complaints, data = attitude)
summary(model.final)


## 3단계 : 독립변수는 종속변수에게 어떠한 영향을 주는가?
##              Estimate Std.
##  complaints     0.75461  

## 독립변수의 회귀계수(Coefficient of Regression) : 0.754
## 독립변수의 기본단위가 1 증가하면
## 종속변수는 약 0.755 정도 증가한다.

## complaints가 1점 증가하면 
## rating은 약 0.755 점 증가한다.


## 4단계 : 예측(prediction)
## dist = -17.5791 + 3.9324*speed
## predict(회귀분석결과, 
##         newdata=data.frame(speed=, 독립변수들),
##         interval=)
predict(cars.lm, 
        newdata = data.frame(speed=200),
        interval = "predict")
## interval => 구간추정 / 없으면 점추정


## 5단계 : 회귀모형의 설명력 = 독립변수의 설명력
## dist가 다른데 speed가 과연 몇%만큼 설명하나? => R-squared(결정계수)
## R-squared:  0.6811 => 0.681*100(%) => 68.1%
## complaints가 rating의 총제곱합(다름)을 약 68.1% 설명한다.
## Adjusted R-squared:  0.670



## 현재 시점으로 보면 해당 모델이 단순선형회귀지만 
## 최종 회귀모형에 독립변수가 2개 이상 포함이 되면
## 1. 회귀계수의 해석
## 독립변수1은 나머지 독립변수들이 고정되어 있을 때에 
## => 나머지 변수들이 가만히 통제 될 때
## 독립변수1의 기본단위가 1 증가하면 종속변수는 약 얼마 증가/감소한다.

## 2. 다중공선성(Multicollinearity)을 확인
## 가정 : 독립변수들 간의 선형의 관계는 없어야 한다.
## VIF : 10 이상이면 다중공선성이 존재한다고 판단
##       독립변수들 간에 선형의 관계가 존재함
##       만약에 이런 결과가 나오면 독립변수들 중에 뺴는 것을 검토
## VIF(varaince inflation factor)
vif(attitude.lm)

## 3. 회귀모형의 설명력 => Adjusted R-squared
## R^2의 단점은 독립변수가 늘어날수록 서서히 늘어난다.
## Adjusted는 독립변수가 들어올 시 종속변수에 영향을 주면 늘어남.

## 4. 독립변수들의 영향력 크기 비교
## 표준화된 회귀계수 => 모든 독립변수의 단위가 동일해야 한다.
## lm.beta::lm.beta(회귀분석결과)
lm.beta::lm.beta(attitude.lm)

rm(list=ls())

##############################################################
### 작 성 자 : JungChul HA                                 ###
##############################################################

# 회귀분석 : 원인과 결과

### 01. 단순회귀분석
## 변수 하나가 다른 변수에 영향을 얼마나 미치는가?
## ex) 광고비와 매출액 간에 인과관계를 알 수 있을까?
## 광고비는 매출액에 얼마나 영향을 미칠 것인가?
## 2001~2016 광고비를 통해 2016의 매출액을 예상

## 1. 독립변수(dependent variable)
# 처음 부터 발생해서 다른 변수에 '영향을 미치는' 변수
# 어떠한 영향을 미칠지 자기 혼자서 결정

## 2. 종속변수(independent variable)
# 설명이 되거나, 결과과 되는 다른 변수의 '영향을 받는' 변수

# 독립변수 ---> 종속변수
# 종속변수는 독립변수에 의해 결정된다.

## 단순회귀분석(simple regression analysis)는
# 독립변수 X가 종속변수 Y에 미치는 영향을 회귀식(회귀방정식)을 이용하여 분석하는 방법
# 하나의 독립변수! => 단순회귀분석
# 회귀식을 가지고 뭘 할꺼야?
# X(원인)에 대해 Y(결과)가 어떻게 될지 추정, 예측을 할거야!


## 자연과학 vs 사회과학
# 자연과학
# X와 Y가 거의 대부분 1:1로 발생을 한다
# f(x) = bo + b1x1
# 기울기는 b1

# 사회과학
# 조건부 평균 
# E(Y|X) = bo + b1X1
# 매출액에 영향을 미치는게 광고비만 있는게 아니다.
# 직원 능력, 경기 상황 등 다양한 요인들이 존재
# 다양한 표본들에 대한 평균식 => 회귀방정식을 찾는 것
# 과연 E(Y|X)가 데이터를 잘 설명 할 수 있을까?
# 차이가 존재한다. => 잔차(residual)
# 잔차(residual)
# Y = b0 + b1X1 + E(앱실론:잔차)
# 잔차는 조건부 평균에서 발생하게 되는데
# 어떤 특정한 변화값이 아니라서 특정값이 아니다.

## 최소자승법(method of least squares) 혹은 최소제곱법
# 실제 데이터를 측정하다 보면 독립변수에 따라 종속변수의 변화하는 정도가 다르게 나타나는 경우가 있는데, 이러한 개별 측정치들 간에 차이
# 표본에서 모 회귀식을 추정한다
# 모집단의 회귀식 
# 표본의 회귀식 + 잔차를 하면 모집단의 회귀식에 가깝게 될 것
# 잔차를 더해주는것 = 최소자승법
# 최소자승법 : 잔차의 제곱합을 최소로 하는방법
# 데이터와 회귀식 간에는 +, - 잔차가 발생하게 되는데
# 그냥 더할시 0이라는 값이 나올 수 있다.
# 이러한 잔차들을 제곱하여 더한다면 잔차들의 합이 발생하게되고
# 여기서 나오는 잔차들의 제곱합이 최소가 되는 회귀식을 찾아야함.
# => 표본을 잘 나타내는 표본회귀식이 될 것.


## 최대우도법(maximum likelihood method)
# 최소자승법은 잔차(오차)에 대해 제곱합을 하여 최소가 되는 회귀식을 찾는 것
# 최대우도법은 어떠한 함수가 최대한 모수가 갖는 함수에 가깝게 하는 것
# 우도 : 회귀식이 측정치를 아주 잘 설명 할 수 있도록 하는 가능성을 최대한으로 올리는 것
# 우도법은 가능성을 최대한 끌어 올리는 것 => 최소자승법과 반대되는 개념
# 표본회귀식이 변화하면서 모 회귀식을 계속 쫓아가는 것
# 최소자승법은 확률분포곡선을 바탕으로 오차를 최소화
# 최대우도법은 측정치를 바탕으로 확률분포곡선을 잘 설명하는 함수의 우도를 최대화
# 반대로 확인하지만 최소자승법과 최대우도법은 같은 값을 가지게 된다.
# 우도라는 것은 측정치를 잘 설명할 수 있는 가능성 => 모 회귀식을 잘 설명하는!


## 가우스-마코프 정리(Gauss-markov Theorem)
# 최소자승법이 가장 좋다!
# 1) 독립변수는 random 하지 않다
# 2) E( E(잔차) ) = 0   // 잔차에 대한 기댓값이 0
# 3) E( 잔차Ei, 잔차Ej) = 0이고 등분산성
# 최소자승법은 BLUE
# B(best) : 추정치의 분산이 최소가 되는것
# L(Linear) : 선형모델을 따르고
# U(unbiased) : 특정값에 편중되지 않는것
# E(estimate) : 추정량
# 최적화 되어 있으면서도, 선형이 나타나야 하고, 어느값에 치주치지 않는 값이 가장 좋다.


## 적합도 검정(goodness of fit test)
# 도출된 회귀식이 얼마나 데이터를 잘 설명해주는가?
# 설명력을 확인해야 한다.
# 표본에 대한 회귀선의 설명력 (R^2) 
# => 추정된 회귀식이 얼마나 측정된 데이터와 일치하는지?
# R^2 => 0~1 => 0.98이라면 98%의 설명력이 있다.
# R^2 = 1 : 100% 설명력. 회귀선안에 모든 관측치가 pitting
# R^2 = 0 : 어쩌다가 몇개만 pitting

# SST(총편차) : 관측점과 평균간의 차이
# SSR(설명되는 편차) : 회귀식과 평균간의 차이
# 설명되는 편차 : 평균과 회귀식 사이에 들어오기 때문.
# SSE(설명되지 않는 편차) : 관측점과 회귀식간의 차이

# 설명되는 편차 / 총편차 = R^2
# R^2(결정계수) = SSR/SST
# 회귀분석에서 분산분석이 나타나는 이유
# SST의 자유도 : n-1
# SSR의 자유도 : 1
# SSE의 자유도 : n-2
# 평균오차제곱 : 